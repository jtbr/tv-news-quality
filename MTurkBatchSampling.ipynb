{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark and save training and holdout sets of snippets in the database\n",
    "\n",
    "Some of the sets are generated using gsdmm for clustering by topic (in two rounds).\n",
    "\n",
    "The sets are then written to disk for labeling via Mechanical Turk (Mturk)\n",
    "\n",
    "To run on demon; written in python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mark a simple random holdout & training sets in the db\n",
    "# (only needs be run once)\n",
    "import pymongo\n",
    "\n",
    "MONGODB_PORT = 25541\n",
    "\n",
    "with pymongo.MongoClient('localhost', MONGODB_PORT) as mclient:\n",
    "    mdb = mclient.snippetdb\n",
    "    msnippets = mdb.snippets\n",
    "    msnippets.create_index([('holdout', pymongo.ASCENDING)])\n",
    "\n",
    "    # add a station-stratified random sample to the holdout set (40*13 = 520)\n",
    "    #msnippets.update_many({\"station_rand_idx\": {\"$lt\": 40}}, { '$set': {\"holdout\": 1} })\n",
    "    # add station-stratified random sample training set (in 4 batches of size 520 each)\n",
    "    msnippets.update_many({\"station_rand_idx\": {\"$gte\": 40, \"$lt\": 80}}, { '$set': {\"train_set\": 0} })\n",
    "    msnippets.update_many({\"station_rand_idx\": {\"$gte\": 80, \"$lt\": 120}}, { '$set': {\"train_set\": 1} }) \n",
    "    msnippets.update_many({\"station_rand_idx\": {\"$gte\": 120, \"$lt\": 160}}, { '$set': {\"train_set\": 2} })\n",
    "    msnippets.update_many({\"station_rand_idx\": {\"$gte\": 160, \"$lt\": 200}}, { '$set': {\"train_set\": 3} })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 56 small clusters into 1, leaving 57 of 113 clusters.\n",
      "cluster 0 of size 271\n",
      "\n",
      "cluster 1 of size 2124\n",
      "\n",
      "cluster 2 of size 1277\n",
      "\n",
      "cluster 3 of size 1428\n",
      "\n",
      "cluster 4 of size 32\n",
      "\n",
      "cluster 5 of size 322\n",
      "\n",
      "cluster 6 of size 142\n",
      "\n",
      "cluster 7 of size 214\n",
      "\n",
      "cluster 8 of size 162\n",
      "\n",
      "cluster 9 of size 1684\n",
      "\n",
      "cluster 10 of size 37\n",
      "\n",
      "cluster 11 of size 37\n",
      "\n",
      "cluster 12 of size 250\n",
      "\n",
      "cluster 13 of size 2345\n",
      "\n",
      "cluster 14 of size 132\n",
      "\n",
      "cluster 15 of size 184\n",
      "\n",
      "cluster 16 of size 3389\n",
      "\n",
      "cluster 17 of size 31\n",
      "\n",
      "cluster 18 of size 85\n",
      "\n",
      "cluster 19 of size 4051\n",
      "\n",
      "cluster 20 of size 36\n",
      "\n",
      "cluster 21 of size 100\n",
      "\n",
      "cluster 22 of size 2323\n",
      "\n",
      "cluster 23 of size 4476\n",
      "\n",
      "cluster 24 of size 283\n",
      "\n",
      "cluster 25 of size 4163\n",
      "\n",
      "cluster 26 of size 4648\n",
      "\n",
      "cluster 27 of size 368\n",
      "\n",
      "cluster 28 of size 3107\n",
      "\n",
      "cluster 29 of size 28\n",
      "\n",
      "cluster 30 of size 487\n",
      "\n",
      "cluster 31 of size 6938\n",
      "\n",
      "cluster 32 of size 2081\n",
      "\n",
      "cluster 33 of size 2248\n",
      "\n",
      "cluster 34 of size 2618\n",
      "\n",
      "cluster 35 of size 1561\n",
      "\n",
      "cluster 36 of size 306\n",
      "\n",
      "cluster 37 of size 41\n",
      "\n",
      "cluster 38 of size 5569\n",
      "\n",
      "cluster 39 of size 299\n",
      "\n",
      "cluster 40 of size 91\n",
      "\n",
      "cluster 41 of size 397\n",
      "\n",
      "cluster 42 of size 144\n",
      "\n",
      "cluster 43 of size 11019\n",
      "\n",
      "cluster 44 of size 1685\n",
      "\n",
      "cluster 45 of size 108\n",
      "\n",
      "cluster 46 of size 8283\n",
      "\n",
      "cluster 47 of size 73\n",
      "\n",
      "cluster 48 of size 53\n",
      "\n",
      "cluster 49 of size 131\n",
      "\n",
      "cluster 50 of size 3395\n",
      "\n",
      "cluster 51 of size 98\n",
      "\n",
      "cluster 52 of size 662\n",
      "\n",
      "cluster 53 of size 681\n",
      "\n",
      "cluster 54 of size 3880\n",
      "\n",
      "cluster 55 of size 2967\n",
      "\n",
      "cluster 56 of size 6115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load clusters generated by GSDMM run and\n",
    "### 0) consolidate custers, combining small & removing empty\n",
    "### 1) mark a small holdout set from those\n",
    "### 2) save the cluster ids to the database\n",
    "### 3) mark training batches\n",
    "###\n",
    "# Note the clusters were formed from snippets having a station_rand_idx>200 (so not in any train_set yet)\n",
    "import pymongo\n",
    "from bson.objectid import ObjectId\n",
    "import numpy\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "MONGODB_PORT = 25541\n",
    "NUM_HOLDOUTS_PER_CLUSTER = 7\n",
    "NUM_TRAIN_PER_CLUSTER = 16\n",
    "\n",
    "# load saved clusters from sampled data\n",
    "output_stem = \"snippet_topicmodel/snippet\"\n",
    "cluster_fn = output_stem + '_gsdmm_doc_clusters_2.pickle'\n",
    "\n",
    "## TEMP: !!!\n",
    "#cluster_fn = output_stem + '_gsdmm_doc_clusters.index_a0.1_b0.1_15its_K326.pickle'\n",
    "\n",
    "with open(cluster_fn, 'r') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# save the members of each cluster\n",
    "cluster_members = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(data['doc_clusters']):\n",
    "    cluster_members[cluster_id].append(idx)\n",
    "\n",
    "nClustersBefore = len(cluster_members.keys())\n",
    "    \n",
    "# sort the indices in each cluster, and combine clusters having too few members\n",
    "for cluster, idxs in cluster_members.items(): # we deliberately want a copy (so not iteritems())\n",
    "    cluster_members[cluster] = sorted(idxs)\n",
    "    if len(idxs) < NUM_HOLDOUTS_PER_CLUSTER + NUM_TRAIN_PER_CLUSTER:\n",
    "        cluster_members[-1] += idxs\n",
    "        del cluster_members[cluster]\n",
    "\n",
    "nClustersAfter = len(cluster_members.keys())\n",
    "print(\"Combined %d small clusters into 1, leaving %d of %d clusters.\" % (nClustersBefore - nClustersAfter, nClustersAfter, nClustersBefore))\n",
    "\n",
    "# renumber the clusters to remove gaps; make the combined cluster be index 0.\n",
    "cluster_members2 = {}\n",
    "new_keys = range( len(cluster_members.keys()) )\n",
    "for new_key, old_key in zip(new_keys, sorted(cluster_members.keys())):\n",
    "    cluster_members2[new_key] = cluster_members[old_key]\n",
    "\n",
    "cluster_members = cluster_members2\n",
    "cluster_members2 = None\n",
    "    \n",
    "# map from idx to _id and insert cluster ids; mark holdouts and train data in mongodb\n",
    "with pymongo.MongoClient('localhost', MONGODB_PORT) as mclient:\n",
    "    mdb = mclient.snippetdb\n",
    "    msnippets = mdb.snippets\n",
    "    #msnippets.update_many({}, {\"$unset\": \"train_set\"})\n",
    "    #msnippets.update_many({}, {\"$unset\": \"recluster\"})\n",
    "    \n",
    "    for cluster, idxs in cluster_members.iteritems():\n",
    "        print(\"cluster %d of size %d\\n\" % (cluster, len(idxs)))\n",
    "        for i, idx in enumerate(idxs):\n",
    "            update_spec = {\"cluster_2\": cluster}\n",
    "            \n",
    "            # assign to holdout or train set as applicable\n",
    "            if i < NUM_HOLDOUTS_PER_CLUSTER:\n",
    "                update_spec[\"holdout\"] = 3\n",
    "            elif i < NUM_HOLDOUTS_PER_CLUSTER + NUM_TRAIN_PER_CLUSTER: \n",
    "                update_spec[\"train_set\"] = 20 + (i-NUM_HOLDOUTS_PER_CLUSTER) / 8  # 10-12 w/ 3 docs per cluster\n",
    "        \n",
    "#             # label the large clusters for re-clustering (exclude what are mostly ads in small clusters)\n",
    "#             if cluster != 0 and len(idxs) >= 100:\n",
    "#                 update_spec[\"recluster\"] = True\n",
    "            \n",
    "            query = {\"_id\": ObjectId(data['doc_ids'][idx])}\n",
    "            \n",
    "            msnippets.update_one(query, {\"$set\": update_spec})\n",
    "            \n",
    "#             if i > len(idxs) - 4:\n",
    "#                 doc = msnippets.find_one(query)\n",
    "#                 print(\"%s\\n%s\\n%s\\n\\n\" % (doc['snippet_part1'], doc['snippet_part2'], doc['snippet_part3']))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to convert from isodate as written below: datetime.datetime.strptime(datestr,\"%Y-%m-%d %H:%M:%S\")\n",
    "# snippets have already had double quotes replaced with single quotes\n",
    "# snippets can be identified in mongodb by ccfn & ccsampnum (filename & file_idx)\n",
    "def writeMTurkBatchFile(docs, filename, batch, holdout):\n",
    "    with open(filename, 'w') as mtbf:\n",
    "        mtbf.write('batch,idx,holdout,ccfn,ccsampnum,station,airdate,priorsample,sample,nextsample\\n')\n",
    "        #TODO: UTF-8 encoding; remove non- UTF-8\n",
    "        for idx, doc in enumerate(docs):\n",
    "            samplerecord = str(batch) + ',' + str(idx) + ',' + str(holdout).lower()  + ',' \\\n",
    "                    + doc['filename'] + ',' + str(doc['file_idx']) + ',' \\\n",
    "                    + doc['station'] + ',' + str(doc['airdatetime']) + ',\"' + doc['snippet_part1'] + '\",\"' \\\n",
    "                    + doc['snippet_part2'] + '\",\"' + doc['snippet_part3'] + '\"\\n'\n",
    "            mtbf.write(samplerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_path = \"mturk_batches/\" # assumed to exist\n",
    "\n",
    "# map from idx to _id and insert cluster ids; mark holdouts and train data in mongodb\n",
    "with pymongo.MongoClient('localhost', MONGODB_PORT) as mclient:\n",
    "    mdb = mclient.snippetdb\n",
    "    msnippets = mdb.snippets\n",
    "    \n",
    "    # save holdouts\n",
    "    for holdout_set in range(1,4):\n",
    "        doc_cursor = msnippets.find({\"holdout\": holdout_set})\n",
    "        writeMTurkBatchFile(doc_cursor, mturk_path + \"holdouts_\" + str(holdout_set) + \".csv\", holdout_set, True)\n",
    "    \n",
    "    # save train sets\n",
    "    for train_set in [0, 1, 2, 3, 10, 11, 12, 20, 21]:\n",
    "        doc_cursor = msnippets.find({\"train_set\": train_set})\n",
    "        writeMTurkBatchFile(doc_cursor, mturk_path + \"train_set_\" + str(train_set) + \".csv\", train_set, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
